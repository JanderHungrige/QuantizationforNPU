{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T12:40:54.386343Z",
     "start_time": "2020-09-23T12:40:49.875627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n",
      "/home/base/Documents/Git/Projekte/Quantization\n",
      "I Am  none\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "print(tf.version.VERSION)\n",
    "if tf.__version__.startswith('1.15'):\n",
    "    # This prevents some errors that otherwise occur when converting the model with TF 1.15...\n",
    "    tf.enable_eager_execution() # Only if TF is version 1.15\n",
    "from keras_vggface_TF.vggfaceTF import VGGFace\n",
    "print(os.getcwd())\n",
    "pretrained_model = VGGFace(model='resnet50', include_top=True, input_shape=(224, 224, 3), pooling='avg')  # pooling: None, avg or max\n",
    "#pretrained_model.summary()\n",
    "#pretrained_model.save('VGGmodelTF')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-23T12:40:56.775607Z",
     "start_time": "2020-09-23T12:40:55.091137Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9021 images belonging to 8555 classes.\n",
      "Found 538 images belonging to 480 classes.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from keras_vggface_TF.utils import preprocess_input\n",
    "\n",
    "base_dir_home=Path('/home/base/Documents/Git/Projekte/CelebFaceMatcher/Finetunedata/')\n",
    "train_dir_home= base_dir_home / 'Train'\n",
    "test_dir_home= base_dir_home / 'Test'\n",
    "\n",
    "\n",
    "#Create Data generatro fwith preprocessed images\n",
    "\n",
    "def Vgg_Preprocess(img):\n",
    "#     /img = img.astype('float32')\n",
    "    #img = expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img, version=2) \n",
    "    return img\n",
    "\n",
    "train_datagen=tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=Vgg_Preprocess)\n",
    "\n",
    "# train_generator = train_datagen.flow_from_directory(\n",
    "#         'data/train',\n",
    "#         target_size=(150, 150),\n",
    "#         batch_size=32,\n",
    "#         class_mode='binary')\n",
    "\n",
    "\n",
    "# Configure the dataset for performance\n",
    "# Use buffered prefetching to load images from disk without having I/O become blocking. To learn more about this method see the data performance guide. https://www.tensorflow.org/guide/data_performance\n",
    "\n",
    "# train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "# validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "# test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "# Create dataset from directory\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (224, 224)\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(train_dir_home,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             target_size=IMG_SIZE,\n",
    "                                             seed=42,\n",
    "                                             )\n",
    "\n",
    "validation_dataset = train_datagen.flow_from_directory(test_dir_home,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             target_size=IMG_SIZE,\n",
    "                                             seed=42,\n",
    "                                             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-23T12:41:08.726Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_model_optimization as tfmot\n",
    "import tensorflow.keras.optimizers\n",
    "\n",
    "\n",
    "\n",
    "#Set which quantisatiomn option you choose. qunatize_model sets everything to default and quantizes the whole model\n",
    "# Set the pretrained model to quantization state, adding fake nodes\n",
    "q_aware_model = tfmot.quantization.keras.quantize_model(pretrained_model)\n",
    "\n",
    "# `quantize_model` requires a recompile before training.\n",
    "# q_aware_model.compile(\n",
    "#     optimizer= tf.keras.optimizers.Adam,\n",
    "#     loss=tf.keras.losses.binary_crossentropy,\n",
    "#     metrics= tf.keras.metrics.Accuracy\n",
    "# )\n",
    "q_aware_model.compile(\n",
    "    optimizer= 'Adam',\n",
    "    loss=tf.keras.losses.binary_crossentropy,\n",
    "    metrics= ['accuracy']\n",
    ")\n",
    "#Train or finetune your model now\n",
    "initial_epochs = 10\n",
    "\n",
    "\n",
    "history = q_aware_model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset)\n",
    "\n",
    "\n",
    "# Now finetune\n",
    "\n",
    "# # Fine-tune from this layer onwards\n",
    "# fine_tune_at = 100\n",
    "\n",
    "# # Freeze all the layers before the `fine_tune_at` layer\n",
    "# for layer in quantize_model.layers[:fine_tune_at]:\n",
    "#   layer.trainable =  False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-15T13:20:33.349Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def representative_dataset_gen():\n",
    "  for _ in range(10):\n",
    "    pfad='/home/base/Documents/Git/Projekte/CelebFaceMatcher/All_croped_images/Aaron Paul/000001.jpg'\n",
    "    img=cv2.imread(pfad)\n",
    "    img = np.expand_dims(img,0).astype(np.float32) \n",
    "    yield [img]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(q_aware_model)\n",
    "    \n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "#converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "converter.inference_input_type = tf.int8 # or tf.uint8\n",
    "converter.inference_output_type = tf.int8 # or tf.uint8\n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "\n",
    "if tf.__version__.startswith('1.'):\n",
    "    open(\"QAT1153_all_int8.tflite\", \"wb\").write(quantized_tflite_model)\n",
    "if tf.__version__.startswith('2.'):\n",
    "    with open(\"QAT220_all_int8.tflite\", 'wb') as f:\n",
    "        f.write(quantized_tflite_model)# mit 220 vs 2_2_0 Ich hatte im modelcode dtype int32 und int8 eingefüght. Jetzt wieder draußen\n",
    "print('I am outahere')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
