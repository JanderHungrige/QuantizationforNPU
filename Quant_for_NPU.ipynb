{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantize your model for NPU of i.mx 8M Plus\n",
    "\n",
    "For more information please read here: https://towardsdatascience.com/quantize-your-deep-learning-model-to-run-on-an-npu-2900191757e5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pathlib as Path\n",
    "\n",
    "tfVersion=tf.version.VERSION.replace(\".\", \"\")# use for later savename\n",
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Representative Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folderpath='./All_croped_images/'\n",
    "\n",
    "def prepare(img):\n",
    "    img = np.expand_dims(img,0).astype(np.float32)\n",
    "    img = preprocess_input(img, version=2)\n",
    "    return img\n",
    "      \n",
    "repDatagen=tf.keras.preprocessing.image.ImageDataGenerator(preprocessing_function=prepare)\n",
    "datagen=repDatagen.flow_from_directory(folderpath,target_size=(224,224),batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load your model here from h5, pb, or import it via Git, or just train the model here.\n",
    "Example here with a saved h5 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the h5 model \n",
    "pretrained_model = tf.keras.models.load_model('my_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now set the Converter Depending on your TF Version and how you load the model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Use a Loaded Keras Model\n",
    "with any version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_keras_model(pretrained_model) # Works only with TF1.x       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load frome h5 file\n",
    "### TF 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(saved_model_dir + h5_modelname) #works now also with TF2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF < 2.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Then use the converter as before\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model_file(pretrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from saved model (pb) file\n",
    "\n",
    "### TF 2.x \n",
    "Here we recommend not to use any older version and the save and freeze graph method. The pb files of TF < 1.15 are fundamentally differnet from the ones TF >1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) # the folder contains the pb file and a assets and variables folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Fully Convert the Model with either in- and output int8 or fLoat32  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_gen\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.experimental_new_converter = True\n",
    "\n",
    "converter.target_spec.supported_types = [tf.int8]\n",
    "converter.inference_input_type = tf.int8 \n",
    "converter.inference_output_type = tf.int8 \n",
    "quantized_tflite_model = converter.convert()\n",
    "\n",
    "open('quant_model.tflite' , \"wb\").write(quantized_tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
